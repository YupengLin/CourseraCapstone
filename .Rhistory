runExample("03_reactivity")
runExample("01_hello")
runApp("DataProduct")
runApp("DataProduct", display.mode = "showcase")
t <- group_by(mtcars, carb) %>% summarize(mean=mean(mpg))
qplot(x=carb, y=mean, data=t, geom="bar", stat="identity",position="dodge")
t <- group_by(mtcars, gear) %>% summarize(mean=mean(mpg))
qplot(x=gear, y=mean, data=t, geom="bar", stat="identity",position="dodge")
t <- group_by(mtcars, vs) %>% summarize(mean=mean(mpg))
qplot(x=vs, y=mean, data=t, geom="bar", stat="identity",position="dodge")
t <- group_by(mtcars, cyl) %>% summarize(mean=mean(mpg))
qplot(x=cyl, y=mean, data=t, geom="bar", stat="identity",position="dodge")
t <- group_by(mtcars, cyl) %>% summarize(mean=mean(mpg))
qplot(x=cyl, y=mean, data=t, geom="bar", stat="identity",position="dodge")
runApp("DataProduct", display.mode = "showcase")
min(mtcars$wt)
max(mtcars$wt)
min(mtcars$disp)
max(mtcars$disp)
?mtcars
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
level(mtcars$cyl)
levels(mtcars$cyl)
str(mtcars)
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
d <- data.frame()
d$a <- 1
d$a <- c("1")
d
colNames(d)
colName(d)
colNames(d)
colnames(d)
colnames(d) <- c("a","k","c")
colnames(d)[1:3] <- c("a","k","c")
nodata <- data.frame(disp= numeric(0), wt= numeric(0), cyl = numeric(0))
min(mtcars$hp)
max(mtcars$hp)
runApp("DataProduct", display.mode = "showcase")
para <- data.frame(disp=numeric(0), hp=numeric(0),
wt=numeric(0),cyl=numeric(0),am=numeric(0),vs=numeric(0))
para
para$disp <- 1
para[1,1] <- 1
para
para <- data.frame(disp=numeric(0), hp=numeric(0),
wt=numeric(0),cyl=numeric(0),am=0,vs=1)
runApp("DataProduct", display.mode = "showcase")
print(para)
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runExample("03_reactivity")
library(caret)
modFit <- train(mpg ~ disp+hp+wt+cyl+am+vs,method="glm",data=mtcars)
runApp("DataProduct", display.mode = "showcase")
c <- "1"
c
as.numeric(c)
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
runApp("DataProduct", display.mode = "showcase")
require(devtool)
require(devtools)
install.packages("devtools")
library(devtools)
require(devtools)
install_github("slidify", "ramnathv")
install_github("slidifyLibraries", "ramnathv")
library(slidify)
install.packages('devtools')
library(devtools)
install_github('rstudio/shinyapps')
library(shinyapps)
library(shiny)
install_github('rstudio/shinyapps')
sessionInfo()
install.packages("forcast")
install.packages("forecast")
library(forecast)
install.packages("tseries")
?predict
install.packages("forecast")
library(forecast)
library(lubridate)
install.packages("lubridate")
library(lubridate)
dat <- read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
?bats
est1 <- bats(tstrain)
?forecast
fcast <- forecast(est1,level=95)
accuracy(fcast, ts(testing$visitsTumblr))
tstesting = ts(testing$visitsTumblr)
accracy(fcast, tstesting)
accuracy(fcast, tstesting)
tsting
tstesting
tstrain
fcast
est1
?bats
length(fcast)
fcast <- forecast(est1)
length(fcast)
View(dat)
?forecoast
?forecast
fcast <- forecast(est1,tstesting,level=95)
fcast <- forecast(est1,nrows(tstesting),level=95)
fcast <- forecast(est1,length(tstesting),level=95)
accuracy(fcast, tstesting)
fcast
accuracy(fcast, tstesting)
fcast <- forecast(est1,nrows(testing),level=95)
fcast <- forecast(est1,nrow(testing),level=95)
accuracy(fcast, testing)
accuracy(fcast, tstesting)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
plot.enet(fit)
fit
?plot.enet
plot(fit)
par(mfrow=c(2,2))
plot(fit)
plot.enet(fit)
par(mfrow=c(4,2))
plot(fit)
x <- c(1,2,4,0)
y <- c(0.5,1,2,0)
plot(x,y)
?plot
x
y
plot(x,y)
library(RWeka)
source('~/Documents/datasciencecoursera/capstone.R')
source('~/Documents/datasciencecoursera/capstone.R')
source('~/Documents/datasciencecoursera/capstone.R')
source('~/Documents/datasciencecoursera/capstone.R')
blog_corpus <- tm_map(blog_corpus, toSpace, "/")
blog_corpus <- tm_map(blog_corpus,to_space,"@")
blog_corpus <- tm_map(blog_corpus,to_space,"\\|")
blog_corpus <- VCorpus(VectorSource(line_blog))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
blog_corpus <- tm_map(blog_corpus, toSpace, "/")
blog_corpus <- tm_map(blog_corpus,to_space,"@")
blog_corpus <- tm_map(blog_corpus, toSpace, "/")
blog_corpus <- VCorpus(VectorSource(line_blog))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
blog_corpus <- tm_map(blog_corpus, toSpace, "/")
blog_corpus <- tm_map(blog_corpus, toSpace, "@")
blog_corpus<- tm_map(blog_corpus, toSpace, "\\|")
source('~/Documents/datasciencecoursera/capstone.R')
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
as.character(blog_corpus[[1]])
tdm <- TermDocumentMatrix(blog_corpus)
inspect(tdm[1:5,1:5])
options(mc.cores=1)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
library(parallel)
tdm <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}
tdm <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
source('~/Documents/datasciencecoursera/capstone.R')
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
options(mc.cores=1)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(blog_corpus, control = list(tokenize = BigramTokenizer))
require(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm <- TermDocumentMatrix(blog_corpus, control = list(tokenize = TrigramTokenizer))
BigramTokenizer  <- function(x) {
RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 3))
}
myTdmBi.d <- TermDocumentMatrix(
blog_corpus,
control = list(tokenize = BigramTokenizer, weighting = weightTfIdf)
)
library(rJava)
BigramTokenizer  <- function(x) {
RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 3))
}
myTdmBi.d <- TermDocumentMatrix(
blog_corpus,
control = list(tokenize = BigramTokenizer, weighting = weightTfIdf)
)
library(rJava)
WPM("load-package", "Decorate")
make_Weka_classifier("Decorate")
install.packages("rJava")
install.packages("rJava")
library(rJava)
BigramTokenizer  <- function(x) {
RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 3))
}
myTdmBi.d <- TermDocumentMatrix(
blog_corpus,
control = list(tokenize = BigramTokenizer, weighting = weightTfIdf)
)
source('~/Documents/datasciencecoursera/capstone.R')
source('~/Documents/datasciencecoursera/capstone.R')
BigramTokenizer  <- function(x) {
RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 3))
}
myTdmBi.d <- TermDocumentMatrix(
blog_corpus,
control = list(tokenize = BigramTokenizer, weighting = weightTfIdf)
)
BigramTokenizer  <- function(x) {
RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 3))
}
myTdmBi.d <- TermDocumentMatrix(
blog_corpus,
control = list(tokenize = BigramTokenizer)
)
sessionInfo()
install.packages("Snowball")
install.packages("SnowballC")
library(SnowballC)
BigramTokenizer  <- function(x) {
RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 3))
}
myTdmBi.d <- TermDocumentMatrix(
blog_corpus,
control = list(tokenize = BigramTokenizer)
)
BigramTokenizer  <- function(x) {
RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 3))
}
myTdmBi.d <- TermDocumentMatrix(
blog_corpus
)
source('~/Documents/datasciencecoursera/capstone.R')
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(blog_corpus, control = list(tokenize = TrigramTokenizer))
options(mc.cores=1)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(blog_corpus, control = list(tokenize = TrigramTokenizer))
filePath_blog <- "final/en_US/en_US.blogs.txt"
filePath_news <- "final/en_US/en_US.news.txt"
filePath_twitter <- "final/en_US/en_US.twitter.txt"
conn_blog <- file(filePath_blog)
conn_news <- file(filePath_news)
conn_twitter <- file(filePath_twitter)
set.seed(1234)
blogs <- readLines(conn_blog)
blog_index <- c(1:length(blogs))
setwd("capstone")
filePath_blog <- "final/en_US/en_US.blogs.txt"
filePath_news <- "final/en_US/en_US.news.txt"
filePath_twitter <- "final/en_US/en_US.twitter.txt"
conn_blog <- file(filePath_blog)
conn_news <- file(filePath_news)
conn_twitter <- file(filePath_twitter)
set.seed(1234)
blogs <- readLines(conn_blog)
blog_index <- c(1:length(blogs))
blog_length
length(blog_index)
899288*0.002
round(0.5*4)
blog_training_index <- sample(blog_index, round(length(blog_index) * 0.002))
source('~/Documents/datasciencecoursera/capstone/capstone.R')
source('~/Documents/datasciencecoursera/capstone/capstone.R')
View(three_tokens)
View(two_tokens)
freq_two <- sort(two_tokens, decreasing = T)
View(freq_two)
freq_two <- sort(table(two_tokens), decreasing = T)
View(freq_two)
length(two_tokens)
length(three_tokens)
freq_three <- sort(table(three_tokens), decreasing = T)
View(freq_three)
View(freq_three)
?da.dateframe
?as.dateframe
?as.dataframe
freq_two <- as.data.frame(freq_two)
rowName(freq_two)
rowNames(freq_two)
names(freq_two)
View(freq_two)
row.names(freq_two)
col.names(freq_two)
colnames(freq_two)
table[1,1]
freq_two$row.names
freq_two <- sort(table(two_tokens), decreasing = T)
View(freq_two)
row.names(freq_two)
freq_two$x
freq_two[1]
freq_two[2]
freq_two[,2]
freq_two[2]
freq_two[[2]]
freq_two[[1]]
f2 <- as.data.frame(freq_two)
str(f2)
View(f2)
f2$freq_two
names(freq_two)
count <- as.vector(freq_two)
View(count)
stat_two <- data.frame(word = row.names(freq_two), count = as.vector(freq_two))
View(stat_two)
stat_two$prob <- stat_two$count / length(two_token)
stat_two$prob <- stat_two$count / length(two_tokens)
View(stat_two)
View(stat_three)
source('~/Documents/datasciencecoursera/capstone/algo.R')
freq_two <- sort(table(two_tokens), decreasing = T)
stat_two <- data.frame(word = row.names(freq_two), count = as.vector(freq_two))
stat_two$prob <- stat_two$count / length(two_tokens)
View(three_tokens)
req_three <- sort(table(three_tokens), decreasing = T)
freq_three <- sort(table(three_tokens), decreasing = T)
stat_three <- data.frame(word= row.names(freq_three, count = as.vector(freq_three)))
source('~/Documents/datasciencecoursera/capstone/algo.R')
View(three_tokens)
save(one_token,two_tokens, three_tokens, file = "token_one_percent.RData")
grep("a case", freq_three$word)
grep("a case", stat_three$word)
c <- grep("a case", stat_three$word)
stt_three[c,]
stat_three[c,]
c <- grep("would mean", stat_three$word)
stat_three[c,]
c <- grep("would", stat_three$word)
stat_three[c,]
c <- grep("mean", stat_three$word)
stat_three[c,]
getwd()
length(blogs)
blog_training_set[1:10]
as.character(blog_corpus[1])
as.character(blog_corpus[[2])
as.character(blog_corpus[[2]])
as.character(blog_corpus[[3]])
as.character(blog_corpus[[4]])
head(stat_three)
head(stat_two)
View(stat_two)
freq_one <- sort(table(one_token), decreasing=T)
stat_one <- data.frame(word = row.names(freq_one), count = as.vector(freq_one))
stat_one$prob <- stat_one$count / length(one_tokens)
freq_one <- sort(table(one_token), decreasing=T)
stat_one <- data.frame(word = row.names(freq_one), count = as.vector(freq_one))
stat_one$prob <- stat_one$count / length(one_token)
wordCloud(stat_one$word, stat_one$count)
library(tm)
wordCloud(stat_one$word, stat_one$count)
wordcloud(stat_one$word, stat_one$count)
library(ggplot2)
wordcloud(stat_one$word, stat_one$count)
wordcloud(words = stat_one$word, freq = stat_one$count, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
?wordcloud
install.packages("wordcloud")
library(wordcloud)
wordcloud(words = stat_one$word, freq = stat_one$count, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
View(stat_one)
training_set <- c(blog_training_set)
blog_corpus <- VCorpus(VectorSource(training_set))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
blog_corpus <- tm_map(blog_corpus, toSpace, "/")
blog_corpus <- tm_map(blog_corpus, toSpace, "@")
blog_corpus<- tm_map(blog_corpus, toSpace, "\\|")
blog_corpus <- tm_map(blog_corpus,content_transformer(tolower))
blog_corpus <- tm_map(blog_corpus,removeNumbers)
blog_corpus <- tm_map(blog_corpus,removePunctuation)
blog_corpus <- tm_map(blog_corpus,removeWords, stopwords("english"))
blog_corpus <- tm_map(blog_corpus, stripWhitespace)
sentence <- ""
for (i in 1:length(training_set)){
sentence <- paste(sentence, as.character(blog_corpus[[i]]))
}
tokens <- unlist(strsplit(sentence," "))
tokens <- tokens[tokens != ""]
two_tokens <- c("")
three_tokens <-c("")
for(i in 1:length(training_set)){
sentence <- ""
sentence <- paste(sentence, as.character(blog_corpus[[i]])) #ingest single sentence
one_token <- unlist(strsplit(sentence," "))
one_token <- one_token[one_token != ""] }
freq_one <- sort(table(one_token), decreasing=T)
stat_one <- data.frame(word = row.names(freq_one), count = as.vector(freq_one))
stat_one$prob <- stat_one$count / length(one_token)
wordcloud(words = stat_one$word, freq = stat_one$count, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
training_set <- c(blog_training_set, news_training_set, twitter_training_set)
blog_corpus <- VCorpus(VectorSource(training_set))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
blog_corpus <- tm_map(blog_corpus, toSpace, "/")
blog_corpus <- tm_map(blog_corpus, toSpace, "@")
blog_corpus<- tm_map(blog_corpus, toSpace, "\\|")
blog_corpus <- tm_map(blog_corpus,content_transformer(tolower))
blog_corpus <- tm_map(blog_corpus,removeNumbers)
blog_corpus <- tm_map(blog_corpus,removePunctuation)
blog_corpus <- tm_map(blog_corpus,removeWords, stopwords("english"))
blog_corpus <- tm_map(blog_corpus, stripWhitespace)
sentence <- ""
for (i in 1:length(training_set)){
sentence <- paste(sentence, as.character(blog_corpus[[i]]))
}
tokens <- unlist(strsplit(sentence," "))
tokens <- tokens[tokens != ""]
two_tokens <- c("")
three_tokens <-c("")
for(i in 1:length(training_set)){
sentence <- ""
sentence <- paste(sentence, as.character(blog_corpus[[i]])) #ingest single sentence
one_token <- unlist(strsplit(sentence," "))
one_token <- one_token[one_token != ""] }
freq_one <- sort(table(one_token), decreasing=T)
stat_one <- data.frame(word = row.names(freq_one), count = as.vector(freq_one))
stat_one$prob <- stat_one$count / length(one_token)
View(stat_one)
freq_one <- sort(table(one_token), decreasing=T)
stat_one <- data.frame(word = row.names(freq_one), count = as.vector(freq_one))
stat_one$prob <- stat_one$count / length(one_token)
View(stat_one)
View(freq_two)
c <- grep("\.", stat_two$word)
c <- grep("\.", stat_two$word)
c <- grep(".", stat_two$word)
c
head(c)
head(c,20)
c <- grep("\w+(?=(\.))", stat_two$word)
c <- grep("$\.", stat_two$word)
c <- grep("$.", stat_two$word)
c
c
c <- grep("\.", stat_two$word)
wordcloud(words = stat_one$word, freq = stat_one$count, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
View(stat_one)
View(freq_one)
View(one_token)
View(training_set)
sentence <- ""
for (i in 1:length(training_set)){
sentence <- paste(sentence, as.character(blog_corpus[[i]]))
}
?load
load("token_one_percent.RData")
View(tokens)
freq_one <- sort(table(tokens), decreasing=T)
stat_one <- data.frame(word = row.names(freq_one), count = as.vector(freq_one))
stat_one$prob <- stat_one$count / length(tokens)
View(stat_one)
wordcloud(words = stat_one$word, freq = stat_one$count, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
head(stat_one)
head(stat_two)
View(stat_two)
head(stat_three)
head(stat_three,10)
head(stat_three,20)
head(stat_two, 10)
