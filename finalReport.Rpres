Final Report of Coursera Capstone Project
========================================================
author: Yupeng Lin
date: 08/20/2015

The Goal and Usage of Capstone Project
========================================================

#### Goal
The coursera capstone project aims to build a predictive model for user input. The training set comes from three corpus extracted from blog, news and twitter. The prediction is based on word level instead of character level. Therefore, the n-gram and back-off algorithm is used for this project. 

#### Usage
The left side panel is the input section, while the right side panel is the three output options. In order to get the prediction of output, the space after the sentence is expected. This space after the sentence gives the algorithm the signal to begin prediction. Unless if we only input "happy", the systems will be confused between "happy!" and "happy birthday"



The Layout of Capstone Project
========================================================
![](demo1.png)



The Algorithms Used In Project
========================================================

The algorithms in project is n-gram and back-off model.

### N-gram algorithm
I used the text mining package to exact the frequencies of single word, two word n-gram, three word n-gram and four n-gram. Given the n-1 word, we can used the frequency table to find out the highest probability n-th word. Then, I convert the frequency data frame to data table as the hashtable. By this way, the computational time will be reduced.

### Linear Interpolation and Back-off Model
What if the given n-1 word combination is not in the frequency table ? The easiest and straightforward way is linear interpolation.

P(Wi | Wi-1, Wi-2) = 0.6P(Wi | Wi-1, Wi-2) + 0.3P(Wi | Wi-1) + 0.1P(Wi)

Model And Algorithms (Continued)
===============================

The more complex way is back-off model.The estimate for an n-gram is allowed to back off through progressively shorter histories. 
![](backoff1.png)

In this project, I used the simplified back-off model. 1> Search the 4-gram table. 2> if miss in 4-gram table, search the 3-gram table. 3> if miss in 3-gram, search the 2-gram table.


Future Improvement
===============================================

### Character-based Prediction
As we can see from previous page, this app could only analyze the words and sentences based on word, while it cannot predict user's next character. For example, if use types "banana", the algorithm will gives the feedback "bananas" based on character or "banana republic" based on word. If I plan to implement this feature, an unigram tables with more statistical characters is indispensable.

### Bigger Corpus
The accuracy of prediction, to a large degree, depends on the comprehensiveness of corpus table. This is the link to google [n-gram] (https://books.google.com/ngrams) project. We will all be shock how large the google n-gram is. They scan the past 200 years published book. Although we are not supposed to be google. Still, the larger corpus, the better result.


